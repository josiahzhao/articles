
Hadoop The Definitive Guide

1 Hadoop基础
    1.1 遇见Hadoop
        数据！
在开拓者时代，他们使用牛进行繁重的收割，而当一头牛无法收割原木时，他们并没有尝试长出更大的牛。
组合大于继承
Kilobyte〈KB〉=1024 bytes
Megabyte〈MB〉=1024 Kilobytes
Gigabyte〈GB〉=1024 Megabytes
Terabyte〈TB〉=1024 Gigabytes
Petabyte〈PB〉=1024 terabytes
Exabyte〈EB〉=1024 petabytes
Zettabyte〈ZB〉=1024 exabytes
Yottabyte〈YB〉=1024 zettabytes
2020年44ZB
纽交所：4~5TB 每天
facebook 240 billion图片，每个月7PB（一天233TB）
more data usually beats better algorithms
        数据存储与分析
1990年，硬盘1370MB，读取速度4.4MB/s，2010年，硬盘1TB（765倍），读取速度100MB/s（25倍），所以读取速度跟不上存储。写更慢。
可以把数据分100份存储在100个硬盘上，同时读取，就能快100倍，这有两个问题：
如果某一个硬盘坏了怎么办；（RAID，HDFS解决这个问题）
如何合并100个份数据；（MapReduce解决这个问题）
总结：Hadoop为存储和分析提供了可靠的，扩展的平台，并且可以使用普通硬盘，开源。
        搜索你全部的数据
Hadoop能够搜索 全部的数据 批处理 暴力方式 以合理的时间
之前很多不能分析的数据现在都能分析了，带来更多的价值和想法
比如邮件服务器中心建立选址的问题
        不只是批处理
MapReduce无疑是批处理，是离线处理，无法在秒内返回，不适合交互式分析
但是Hadoop已经代表了一个生态，不只是HDFS和MapReduce了
比如Hbase就是第一个用于在线服务的组件

真正变革Hadoop处理模型是YARN的产生：Yet Another Resource Negotiator，在Hadoop2中引入，管理集群资源，允许不只是MapReduce运行在Hadoop集群中
引入了很多的处理模式，比如：
交互式SQL：Impala 和 Hive
迭代式处理：Spark
流式处理：Storm，Spark Streaming
搜索：Solr（底层存储HDFS）
        和其他系统比对
Hadoop不是第一个分布式的数据存储和计算系统，但是有独特的地方。
1 RDBMS
为什么关系型数据库不能做大数据分析
1.1 磁盘查询效率的增长远低于传输效率（网络带宽，可以有1000MB/s），而B-Tree的重点就是依赖磁盘查询
这就决定了，更新小数据量，RDBMS基于B-Tree更快，而更新大批量数据，MapReduce更快
MapReduce更适合全量处理，尤其是ad-hoc的实验性质的计算：一次写，多次读
RDBMS更适合查询和更新小批量数据（有索引）：持续小范围读写更新


在Hadoop引入Hive之后，二者的边界就模糊一些了

1.2 另一个区别：Hadoop MapReduce可以很好的处理半结构化（excel）和非结构化数据（图片，视频）
1.3 RDMBS经常需要对数据进行去重和标准化，这个Hadoop不容易解决，因为Hadoop需要从多个地方读取数据，很难保证去重；但是web log就很适合Hadoop，可以有重复，单纯的记录，并且可以join
1.4 MapReduce是线性的，2倍的数据+2倍的机器=时间不变
2 Grip Computing 网格计算
HPC(high-performance computing)和网格计算，通过API（application program interfaces)和MPI(message passing interface)来将计算任务传达给各个计算节点，而数据来自SAN（shared area netword）
这个适合密集计算，但是不适合读取大量的数据，因为网络带宽很珍贵
但Hadoop不一样，可以本地计算，计算和存储一体，节约了带宽
以及开发模式不一样，网格计算虽然给了开发者很大的灵活性，但需要考虑计算失败和执行顺序的问题
MapReduce就不用
3 Volunteer Computing 志愿计算
全世界范围的机器贡献算力，进行并行计算，每个任务很小，MB级别，计算完后返回；同时发放3个同样的任务，两个通过了才行。
也是密集计算型，不能对带宽有要求
并且对计算时间有要求，需要很短才行
MapReduce可以保障几十个小时的运转
        Hadoop简史
Hadoop是由Apache Lucene（文本搜索）的创始人Doug Cutting创建的。
Hadoop是宝宝给一个黄色大象的起的名字，宝宝很擅长起名字，比如Google




    1.2 MapReduce
        天气数据
每小时一个数据，是半结构化（一行行文件），并且是面向记录的（一条条）
数据源：https://www.ncdc.noaa.gov/data-access， NDCC（national climatic data center）
一个zip是一个监控站一年的数据（每小时一条），一共数万个监控站
每一行数据包含：年份，时间，气温
目标：计算每年的最高气温
    用unix tool分析
先不使用hadoop，轮询每一个文件，存储最大值，耗时42分钟
然后并行跑，每一年并行，这里面有几个问题
1. 每一年的数量不一样，并行的话，可能有的年份很快就完成了，浪费多线程的资源；即使有队列，执行完了之后执行另一个，也无法避免最大的那个年份是最慢的；所以需要能够把一个年也能够拆分
2. 如果一年拆成多份，还需要一个循环，多份里面取最大
3. 这些还都受限于一台物理机，无法扩展到多台机器；如果扩展到多台机器，就会在调度，可靠性上陷入困难，谁来主导调度，有一台机器失败了怎么办
所以，尽管并行是必要的，但是确实很复杂
    使用Hadoop分析数据
        Map & Reduce
Map Function：输入：一行原始数据； 执行：筛选无效数据，提取年份和气温，输出：key=年份，value=气温
机制：会把所有的相同key的内容做聚合后，对key做排序，传入reduce

Reduce Function：输入：key和key对应的一系列的value；输出：最大的value

        Java MapReduce
Map：
如下图所示，Haddop使用自己的类型，LongWritable = Long，Text = String，IntWritable = Integer
输入的key是位移，用不到，输入的value就是每一行的内容
使用Context来写输出：key = year， value = 温度


Reduce
输入类型要和map的输出（context.write）的类型一致，Text key就是年份，Iterable<IntWritable>就是所有的温度
输出就是年份和最大的温度


主程序
跑任务之间，会将任务打成jar包，然后提交任务时hadoop会将jar包自动传输到集群里面
程序通过setJobName设置jar包的名称
input的内容可以是一个文件，或者是一个目录（等于里面的所有文件），可以添加多个inputPath
output内容必须不存在，预防覆盖了一个另外的任务结果

SetOutputKeyClass和SetOutputValueClass是设置Reduce的输出的，如果Map的输出和Reduce的输出一样就不用额外配置了，否则就需要单独设置SetMapOutputKey和Value
整个输入的内容没有配置，用的默认的：TextInputFormat（一行行的文本）

    Scalling Out
        Data Flow
Split
之前都是单机运行的mapreduce，为了扩大规模，需要进行分布式的存储和计算。
一个《MapReduce job》是一个工作的单元，由：输入数据，MapReduce方法，配置这几个构成。
Hadoop将《MapReduce Job》切分为两类任务，Map任务和Reduce任务，这些任务通过YARN进行调度；如果一个任务失败了，则会调度到另外的节点再次运行一遍。
Hadoop将输入分隔成为固定的大小，每个输入叫做Split，每个Split会对应一个Map任务。
并行计算时，将这些任务进行负载均衡是必要的：强大的机器可以执行更多的split。
但是如果一个split过小，则会在split和map任务的建立、管理上花费更多的时间。
一个split的大小默认是128MB，和HDFS的Block的大小一样。

Map数据流
Hadoop能够将并行计算做的很好，不会消耗很多的带宽，因为《数据本地优化技术》（Data locality optimization）
1. 这个split默认使用HDFS对应存储的机器进行Map任务运行，避免使用带宽
2. 如果这个split的所有replica的node都在执行任务，则优先使用同一个机架（rack=42U），也能避免大量的带宽
3. 很少的情况，1、2都不满足，则需要off-rack了，将split传输到别的机架（rack）的node上进行map运行，需要占用网络的带宽
这就说明了，为何split=HDFS block size = 128MB重要了：这样就能保证一个split一定能够被一个节点存储下：如果一个split占用2个block，可能两个block分布在不同的node甚至rack上，造成带宽浪费，不高效。
如下图：第一列，本地计算；第二列，同一个rack计算；第三列，off-rack计算，对应上面的1~3

值得注意的是，map任务的输出是在执行map的node上的，因为：
1. map的输出是中间结果，一旦reduce完成，map的输出就没用了
2. 所以map输出存储在hdfs上（replica）会显得很浪费
3. 如果节点挂了，再跑一遍map即可

Reduce数据流
Reduce并没有《数据本地优化技术》（Data locality optimization），因为它必须要跨网络消耗带宽：
1. 需要汇集所有的map的输出
2. 输出的内容需要存储hdfs，及需要分布式replica，需要消耗带宽
但是，reduce输出内容的第一个replica直接存储在执行reduce的node，尽可能的减少带宽浪费
如下图，是只有一个reduce的时候

当有多个reducer的时候，map会将输出建立partition：
1. 一个reduce job对应一个partition
2. 具体应该有几个partition有对应的方法论
3. 每一个key的所有map记录需要在一个partition里面（否则reduce看不到这个key的全量数据，没有意义，比如需全年最大，如果这一年的所有数据没有给到reduce，就会需要二级reduce）
4. 如何进行partition的分隔，可以自主定义，也可以用默认的，根据key进行hash
如下图，是有多个reduce的时候

注：一次任务可能也并不需要reduce，只用map就行了，这种情况，offnode只会发生在map结果写入hdfs的时候（建立relica），如下图

衍生问题
1. reduce的输出记录在哪里（第一个在执行reduce的节点，后面的就是replica）
2. map的输出记录在哪里（本地，不进入hdfs）
3. map任务和数据的分布关系（优先hdfs数据本地，然后是同一个rack，最后是off-rack）
        合并方法Combiner Functions
很多MapReduce Job受限于网络带宽，所以值得花时间来减少map=>reduce的数据传输
所以Hadoop提供了一个接口，能够让开发者撰写Combiner Functions来优化map的输出，减少传输给reduce的数据
比如，计算每一年最大气温
第一个map的输出：
（1950，0）
（1950，20）
（1950，10）
第二个map的输出：
（1950，25）
（1950，15）
那么正常给reduce的内容是：（1950，[0，10，15，20，25]）（排序+合并）
那么，如果每一个map的输出先做一次取最大，那么给reduce的数据就减少为：
（1950，[20，25]）

当然，取最大可以这样，如果是去平均数，就不行了


如果可以通过Combiner Functions来优化永远是值得考虑的

如下图，使用方式加一行即可：（因为reduce的功能可以是combinefunction）

注：Combiner Functions虽然和reduce很像（接口，逻辑），但是完全不是：
1. 执行在一个split对应的node节点上
2. 需要显示定义
衍生问题：
Combiner Function的作用和执行的位置（优化传输，split对应的map job的    节点）
    1.3 HDFS-The Hadoop Distributed Filesystem
        The Design of HDFS
Hadoop Distributed Filesystem
合适的地方：
1. 非常大的文件：PB
2. 流式获取文件（Streaming data access）：一次写入，多次读取，每次读取会涉及全部或者很大一部分的数据，所以读取全部数据的时间比读取第一条数据的时间更重要
3. 商用硬盘：多厂商的普通硬盘都可以
不合适的地方：
1. 延迟，数十毫秒无法达到（后面的HBase可以）
2. 多文件：一个文件，目录，block都需要150B存储在namenode的内存中，所以100万的文件需要300MB的内存；所以，存储数十亿的文件目前是很困难的
3. 多写，多改：目前主要是一个writer，每个block只支持append的方式追加来写；不适合多个writer和在任意offset修改文件
        HDFS概念
Blocks
操作系统的文件系统有块的概念，比如512B，那么最小的文件，也需要占用512B
HDFS也有块的概念，128MB，大很多，但如果一个文件不到128MB，占用就是实际的，这一点和文件系统的块不一样

为什么HDFS的Block比操作系统的大那么多：为了减少seek寻找的时间，因为HDFS的block比较大，这样当传输一个由多个block组成的大文件的时候，速度基本可以接近硬盘传输速度
一个例子：如果seek的时间是10ms，传输的速度是100MB/s，为了让seek的时间是传输的1%，那么就需要一个block的大小是100MB。

Block的概念优势：
1.     可以存储比单块磁盘更大的文件，通过分块，甚至可以用一个文件，将所有集群的存储都占满。
2. 简化：通过使用block而不是操作系统的文件，简化存储管理：一个是大小管理，固定的大小；一个人元数据管理，权限什么的，都交给其他的系统去做；block就是单纯的数据块
3. 容灾：通过block，可以容易做到replica

Namenodes and Datanodes

引申问题：为什么block的size需要是128MB（减少寻找时间，同时保证寻找时间是传输时间的1%以内计算得到的）
